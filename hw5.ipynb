{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c62caf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "down\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tensorflow.keras import optimizers\n",
    "import numpy as np\n",
    "\n",
    "data = []\n",
    "endata = []\n",
    "chdata = []\n",
    "\n",
    "def chine(string):\n",
    "    for ch in string :\n",
    "        if u'\\u4e00' <= ch <= u'\\u9fff':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "for line in open('testing.json', 'r', encoding='utf-8'):\n",
    "    data.append(json.loads(line))  \n",
    "\n",
    "for i in range(len(data)):\n",
    "    if chine(data[i]['chinese']) == True :\n",
    "        if len(data[i]['chinese']) < 10:\n",
    "            endata.append(data[i]['english'])\n",
    "            chdata.append('@'+data[i]['chinese']+'。')\n",
    "        if len(chdata) == 100 :\n",
    "            break\n",
    "\n",
    "en_vocab = set(''.join(endata))\n",
    "id2en = list(en_vocab)\n",
    "en2id = {c:i for i,c in enumerate(id2en)}\n",
    "\n",
    "# 分別生成中英文字典\n",
    "ch_vocab = set(''.join(chdata))\n",
    "id2ch = list(ch_vocab)\n",
    "ch2id = {c:i for i,c in enumerate(id2ch)}\n",
    "\n",
    "print('down')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efd6ee7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char: Choose a recorder.\n",
      "index: [22, 55, 62, 62, 9, 65, 10, 8, 10, 11, 65, 38, 62, 11, 39, 65, 11, 42]\n",
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_num_data = [[en2id[en] for en in line ] for line in endata]\n",
    "ch_num_data = [[ch2id[ch] for ch in line] for line in chdata]\n",
    "de_num_data = [[ch2id[ch] for ch in line][1:] for line in chdata]\n",
    "\n",
    "print('char:', endata[1])\n",
    "print('index:', en_num_data[1])\n",
    "print(len(en_num_data))\n",
    "len(ch_num_data)\n",
    "# de_num_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b8279b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max encoder length: 49\n",
      "max decoder length: 11\n",
      "index data:\n",
      " [22, 55, 62, 62, 9, 65, 10, 8, 10, 11, 65, 38, 62, 11, 39, 65, 11, 42]\n",
      "one hot data:\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "max_encoder_seq_length = max([len(txt) for txt in en_num_data])\n",
    "max_decoder_seq_length = max([len(txt) for txt in ch_num_data])\n",
    "print('max encoder length:', max_encoder_seq_length)\n",
    "print('max decoder length:', max_decoder_seq_length)\n",
    "\n",
    "# 將數據進行onehot處理\n",
    "encoder_input_data = np.zeros((len(en_num_data), max_encoder_seq_length, len(en2id)), dtype='float32')\n",
    "decoder_input_data = np.zeros((len(ch_num_data), max_decoder_seq_length, len(ch2id)), dtype='float32')\n",
    "decoder_target_data = np.zeros((len(ch_num_data), max_decoder_seq_length, len(ch2id)), dtype='float32')\n",
    "\n",
    "for i in range(len(ch_num_data)):\n",
    "    for t, j in enumerate(en_num_data[i]):\n",
    "        encoder_input_data[i, t, j] = 1.\n",
    "    for t, j in enumerate(ch_num_data[i]):\n",
    "        decoder_input_data[i, t, j] = 1.\n",
    "    for t, j in enumerate(de_num_data[i]):\n",
    "        decoder_target_data[i, t, j] = 1.\n",
    "\n",
    "print('index data:\\n', en_num_data[1])\n",
    "print('one hot data:\\n', encoder_input_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "282eaf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_VOCAB_SIZE = len(en2id)\n",
    "CH_VOCAB_SIZE = len(ch2id)\n",
    "HIDDEN_SIZE = 256\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 20\n",
    "EPOCHS = 250\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee669e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "# ==============encoder=============\n",
    "encoder_inputs = Input(shape=(None, EN_VOCAB_SIZE))\n",
    "#emb_inp = Embedding(output_dim=HIDDEN_SIZE, input_dim=EN_VOCAB_SIZE)(encoder_inputs)\n",
    "encoder_h1, encoder_state_h1, encoder_state_c1 = LSTM(HIDDEN_SIZE, return_sequences=True, return_state=True)(encoder_inputs)\n",
    "encoder_h2, encoder_state_h2, encoder_state_c2 = LSTM(HIDDEN_SIZE, return_state=True)(encoder_h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02447ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
